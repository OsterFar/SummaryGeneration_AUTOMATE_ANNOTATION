,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,J01-2004,2001,1,W05-0104,0,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",[369],"The log of the language model score is multiplied by the language model (LM) weight when summing the logs of the language and acoustic scores, as a way of increasing the relative contribution of the language model to the composite score.",Method_Citation
1,J01-2004,2001,2,P08-1013,0,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",[115],"The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.",Method_Citation
2,J01-2004,"Roark, 2001a",4,P04-1015,0,0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank",[347],"The perplexity improvement was achieved by simply taking the existing parsing model and applying it, with no extra training beyond that done for parsing.",Method_Citation
3,J01-2004,"Roark, 2001a",5,P04-1015,0,0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",[408],The improvement that we derived from interpolating the different models above indicates that using multiple models may be the most fruitful path in the future.,Method_Citation
4,J01-2004,2001a,6,P04-1015,0,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",[209],"This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).",Method_Citation
5,J01-2004,2001a,7,P04-1015,0,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",[268],This is an incremental parser with a pruning strategy and no backtracking.,Method_Citation
6,J01-2004,2001a,9,P04-1015,0,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",[356],"The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal, a total of 3,446 words.",Method_Citation
7,J01-2004,"Roark, 2001",10,P05-1022,0,0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",[223],We implement this as a beam search.,Method_Citation
8,J01-2004,"Roark, 2001",11,P05-1022,0,0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search",[297],The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.,Method_Citation
9,J01-2004,"Roark, 2001",12,P05-1022,0,0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",[209],"This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).",Method_Citation
10,J01-2004,"Roark, 2001",13,P04-1006,0,0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children",[268],This is an incremental parser with a pruning strategy and no backtracking.,Method_Citation
11,J01-2004,"Roark, 2001a",14,P05-1063,0,0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",[405],Does a parsing model capture exactly what we need for informed language modeling?,Method_Citation
12,J01-2004,"Roark, 2001",15,W10-2009,0,0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)",[313],"By utilizing a figure of merit to identify promising analyses, we are simply focusing our attention on those parses that are likely to have a high probability, and thus we are increasing the amount of probability mass that we do capture, of the total possible.",Method_Citation
13,J01-2004,2001,17,D09-1034,0,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)",[363],"Splitting of the contractions is critical for parsing, since the two parts oftentimes (as in the previous example) fall in different constituents.",Method_Citation
14,J01-2004,2001,18,D09-1034,0,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",[265],LR and LP are part of the standard set of PARSEVAL measures of parser quality (Black et al. 1991).,Method_Citation
15,J01-2004,2001,19,D09-1034,0,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",[82],This transform has a couple of very nice properties.,Method_Citation
16,J01-2004,2001,20,D09-1034,0,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed",[72],"Let Dw, be the set of all partial derivations for a prefix string 4.",Method_Citation
