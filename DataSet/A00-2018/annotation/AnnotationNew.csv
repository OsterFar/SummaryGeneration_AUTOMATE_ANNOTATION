,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,A00-2018,"Charniak, 2000",2,N10-1002,0,0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)",[117],"Also, the earlier parser uses two techniques not employed in the current parser.",Method_Citation
1,A00-2018,"Charniak, 2000",3,W11-0610,0,0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank",[5],"We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",Method_Citation
2,A00-2018,"Charniak, 2000",4,W06-3119,0,0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus",[9],The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.,Method_Citation
3,A00-2018,"Charniak, 2000",5,N03-2024,0,0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)",[10],"That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w).",Method_Citation
4,A00-2018,"Charniak, 2000",6,N06-1039,0,0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article",[5],"We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.",Method_Citation
5,A00-2018,2000,7,C04-1180,0,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)",[8],"That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.",Method_Citation
6,A00-2018,"Charniak, 2000",8,W05-0638,0,0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)",[117],"Also, the earlier parser uses two techniques not employed in the current parser.",Method_Citation
7,A00-2018,"Charniak, 2000",9,P05-1065,0,0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis",[117],"Also, the earlier parser uses two techniques not employed in the current parser.",Method_Citation
8,A00-2018,"Charniak, 2000",10,P05-1065,0,0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus",[40],In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.,Method_Citation
9,A00-2018,2000,11,P04-1040,0,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows",[108],"As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.",Method_Citation
10,A00-2018,2000,12,P04-1040,0,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees",[134],"As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.",Method_Citation
11,A00-2018,2000,13,P04-1040,0,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",[153],"Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp.",Method_Citation
12,A00-2018,2000,17,N06-1022,0,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents",[172],"As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.",Method_Citation
13,A00-2018,2000,18,N06-1022,0,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))",[172],"As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.",Method_Citation
14,A00-2018,"Charniak, 2000",19,H05-1035,0,0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",[40],In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.,Method_Citation
15,A00-2018,2000,20,P04-1042,0,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size",[142],We believe that two factors contribute to this performance gain.,Method_Citation
