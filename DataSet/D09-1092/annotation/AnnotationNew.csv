,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,D09-1092,"Mimno et al, 2009",1,P14-1004,0,0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs",[186],"Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.",Method_Citation
1,D09-1092,"Mimno et al, 2009",2,P10-1044,0,0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)",[178],"For efficiency, we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.",Method_Citation
2,D09-1092,"Mimno et al, 2009",3,P11-2084,0,0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations",[138],We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.,Method_Citation
3,D09-1092,"Mimno et al, 2009",4,E12-1014,0,0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction",[29],"A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.",Method_Citation
4,D09-1092,"Mimno et al, 2009",5,D11-1086,0,0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)",[157],For each document in the query language we rank all documents in the target language and record the rank of the actual translation.,Method_Citation
5,D09-1092,"Mimno et al, 2009",6,N12-1007,0,0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details",[199],"Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not necessarily reflect those of the sponsor.",Method_Citation
6,D09-1092,2009,7,N12-1007,0,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly",[118],We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.,Method_Citation
7,D09-1092,"Mimno et al, 2009",8,D10-1025,0,0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages",[35],"The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.",Method_Citation
8,D09-1092,"Mimno et al, 2009",9,D10-1025,0,0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)",[41],(3) The graphical model is shown in figure 1.,Method_Citation
9,D09-1092,"Mimno et al, 2009",10,D10-1025,0,0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)",[97],"Rather, these results are intended as a quantitative analysis of the difference between the two models.",Method_Citation
10,D09-1092,"Mimno et al, 2009",11,D10-1025,0,0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)",[27],"Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.",Method_Citation
11,D09-1092,"Mimno et al, 2009",12,D10-1025,0,0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference",[97],"Rather, these results are intended as a quantitative analysis of the difference between the two models.",Method_Citation
12,D09-1092,"Mimno et al, 2009",13,D10-1025,0,0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)",[6],"Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).",Method_Citation
13,D09-1092,"Mimno et al, 2009",15,D10-1025,0,0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours",[86],"Given a set of training document tuples, PLTM can be used to obtain posterior estimates of Φ', ... , ΦL and αm.",Method_Citation
14,D09-1092,"Mimno et al, 2009",16,W12-3117,0,0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)",[150],"In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.",Method_Citation
15,D09-1092,2009,17,W11-2133,0,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)",[151],We are therefore interested in determining whether the information in the document-specific topic distributions is sufficient to identify semantically identical documents.,Method_Citation
16,D09-1092,2009,18,W11-2133,0,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,[99],Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.,Method_Citation
17,D09-1092,"Mimno et al, 2009",19,P14-2110,0,0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic",[77],"Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.",Method_Citation
18,D09-1092,2009,20,P14-2110,0,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language",[3],We introduce a polylingual topic model that discovers topics aligned across multiple languages.,Method_Citation
