,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,W99-0613,"Collins and Singer, 1999",1,N01-1023,0,0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",[16],Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).,Method_Citation
1,W99-0613,"Collins and Singer, 1999",2,N01-1023,0,0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)",[182],Denote the unthresholded classifiers after t â€” 1 rounds by gitâ€”1 and assume that it is the turn for the first classifier to be updated while the second one is kept fixed.,Method_Citation
2,W99-0613,Collins and Singer 1999,3,W03-1509,0,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",[221],We are currently exploring such algorithms.,Method_Citation
3,W99-0613,"Collins and Singer, 1999",4,C02-1154,0,0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus",[38],There has been additional recent work on inducing lexicons or other knowledge sources from large corpora.,Method_Citation
4,W99-0613,"Collins and Singer, 1999",5,C02-1154,0,0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify",[199],Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function.,Method_Citation
5,W99-0613,"Collins and Singer, 1999",6,W06-2204,0,0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",[18],But we will show that the use of unlabeled data can drastically reduce the need for supervision.,Method_Citation
6,W99-0613,"Collins and Singer, 1999",8,W03-1022,0,0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",[244],"Note that on some examples (around 2% of the test set) CoBoost abstained altogether; in these cases we labeled the test example with the baseline, organization, label.",Method_Citation
7,W99-0613,"Collinsand Singer, 1999",9,E09-1018,0,0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",[233],"Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward.",Method_Citation
8,W99-0613,"Collins and Singer, 1999",11,W07-1712,0,0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",[221],We are currently exploring such algorithms.,Method_Citation
9,W99-0613,"Collins and Singer, 1999",12,W09-2208,0,0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",[6],"The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).",Method_Citation
10,W99-0613,"Collins and Singer, 1999",13,W06-2207,0,0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",[46],(Riloff and Jones 99) was brought to our attention as we were preparing the final version of this paper.,Method_Citation
11,W99-0613,"Collins and Singer, 1999",15,W06-2207,0,0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",[186],(8) can now be rewritten5 as which is of the same form as the function Zt used in AdaBoost.,Method_Citation
12,W99-0613,1999,16,P12-1065,0,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,[14],"A contextual rule considers words surrounding the string in the sentence in which it appears (e.g., a rule that any proper name modified by an appositive whose head is president is a person).",Method_Citation
