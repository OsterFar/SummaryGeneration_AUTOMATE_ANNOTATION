,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,A00-2030,"Miller et al., 2000",1,W01-0510,0,0,"Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)",[42],"Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events.",Method_Citation
1,A00-2030,"Miller et al, 2000",2,W01-0510,0,0,"The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences:  in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller",[32],"Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties â€” especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs â€” would also benefit semantic analysis.",Method_Citation
2,A00-2030,"Miller et al, 2000",3,W01-0510,0,0,"The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)",[49],"By necessity, we adopted the strategy of hand marking only the semantics.",Method_Citation
3,A00-2030,"Miller et al, 2000",4,W01-0510,0,0,"One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level",[43],"Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source â€” the Wall Street Journal â€” and is impoverished in articles about rocket launches.",Method_Citation
4,A00-2030,"Miller et al, 2000",5,W01-0510,0,0,"Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3",[40],Further details are discussed in the section Tree Augmentation.,Method_Citation
5,A00-2030,"Miller et al, 2000",6,P14-1078,0,0,"Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns",[16],"For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",Method_Citation
6,A00-2030,2000,7,P05-1061,0,0,"One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations",[58],"Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.",Method_Citation
7,A00-2030,2000,8,P05-1053,0,0,"Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees",[52],"In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.",Method_Citation
8,A00-2030,2000,9,P05-1053,0,0,"Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model",[16],"For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.",Method_Citation
9,A00-2030,2000,10,H05-1094,0,0,"(Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable",[58],"Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.",Method_Citation
10,A00-2030,2000,11,P04-1054,0,0,Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types,[58],"Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.",Method_Citation
11,A00-2030,2000,12,P04-1054,0,0,"WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance",[58],"Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.",Method_Citation
12,A00-2030,"Miller et al, 2000",13,W05-0602,0,0,"The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures",[105],"A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.",Method_Citation
13,A00-2030,"Miller et al, 2000",14,N07-2041,0,0,"Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2",[48],Our annotation staff found syntactic analysis particularly complex and slow going.,Method_Citation
14,A00-2030,2000,15,W10-2924,0,0,Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels,[38],Other labels indicate relations among entities.,Method_Citation
15,A00-2030,"Miller et al, 2000",16,W06-0508,0,0,"Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)",[55],"syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument.",Method_Citation
16,A00-2030,"Miller et al, 2000",17,P07-1055,0,0,"This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)",[38],Other labels indicate relations among entities.,Method_Citation
17,A00-2030,2000,18,W05-0636,0,0,"For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks",[11],"We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).",Method_Citation
18,A00-2030,2000,19,N06-1037,0,0,Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint,[1],"Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.",Method_Citation
