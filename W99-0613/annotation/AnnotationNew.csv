,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,W99-0613,"Collins and Singer, 1999",1,N01-1023,0,0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","[160, 203, 137]","['(5) and ht into Equ.', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']",Method_Citation
1,W99-0613,"Collins and Singer, 1999",2,N01-1023,0,0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","[101, 141, 247]","['An important reason for separating the two types of features is that this opens up the possibility of theoretical analysis of the use of unlabeled examples.', 'For a description of the application of AdaBoost to various NLP problems see the paper by Abney, Schapire, and Singer in this volume.', 'N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree.']",Method_Citation
2,W99-0613,Collins and Singer 1999,3,W03-1509,0,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","[192, 203, 27]","['On each step CoBoost searches for a feature and a weight so as to minimize either 40 or 40.', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).']",Method_Citation
3,W99-0613,"Collins and Singer, 1999",4,C02-1154,0,0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","[254, 40, 44]","['Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.', '(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g., &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.', 'More recently, (Riloff and Jones 99) describe a method they term &quot;mutual bootstrapping&quot; for simultaneously constructing a lexicon and contextual extraction patterns.']",Method_Citation
4,W99-0613,"Collins and Singer, 1999",5,C02-1154,0,0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","[126, 141, 123]","[""The question of what soft function to pick, and how to design' algorithms which optimize it, is an open question, but appears to be a promising way of looking at the problem."", 'For a description of the application of AdaBoost to various NLP problems see the paper by Abney, Schapire, and Singer in this volume.', 'Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem, the assumptions are quite limited.']",Method_Citation
5,W99-0613,"Collins and Singer, 1999",6,W06-2204,0,0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","[8, 250, 157]","['Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.', 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', 'Zt can be written as follows Following the derivation of Schapire and Singer, providing that W+ > W_, Equ.']",Method_Citation
6,W99-0613,"Collins and Singer, 1999",8,W03-1022,0,0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","[141, 230, 108]","['For a description of the application of AdaBoost to various NLP problems see the paper by Abney, Schapire, and Singer in this volume.', 'In our experiments we set the parameter values randomly, and then ran EM to convergence.', 'In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.']",Method_Citation
7,W99-0613,"Collinsand Singer, 1999",9,E09-1018,0,0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","[115, 125, 232]","['The problem can be represented as a graph with 2N vertices corresponding to the members of X1 and X2.', 'It may be more realistic to replace the second criteria with a softer one, for example (Blum and Mitchell 98) suggest the alternative Alternatively, if Ii and 12 are probabilistic learners, it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.', 'For example, the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).']",Method_Citation
8,W99-0613,"Collins and Singer, 1999",11,W07-1712,0,0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","[230, 220, 156]","['In our experiments we set the parameter values randomly, and then ran EM to convergence.', '(7), such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', 'Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.']",Method_Citation
9,W99-0613,"Collins and Singer, 1999",12,W09-2208,0,0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","[91, 27, 203]","['There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).']",Method_Citation
10,W99-0613,"Collins and Singer, 1999",13,W06-2207,0,0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","[46, 223, 199]","['(Riloff and Jones 99) was brought to our attention as we were preparing the final version of this paper.', 'A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on unlabeled examples, and observed variables on (seed) labeled examples.', 'Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function.']",Method_Citation
11,W99-0613,"Collins and Singer, 1999",15,W06-2207,0,0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","[230, 199, 136]","['In our experiments we set the parameter values randomly, and then ran EM to convergence.', 'Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function.', 'We then discuss how we adapt and generalize a boosting algorithm, AdaBoost, to the problem of named entity classification.']",Method_Citation
12,W99-0613,1999,16,P12-1065,0,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,"[73, 79, 203]","['Each xii is a member of X, where X is a set of possible features.', '2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).']",Method_Citation
