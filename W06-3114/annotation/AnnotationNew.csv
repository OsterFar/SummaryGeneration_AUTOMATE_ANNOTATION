,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,W06-3114,"Koehn and Monz, 2006",1,W06-3120,0,0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","[42, 155, 163]","['It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation.', 'For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations.', 'Not every annotator was fluent in both the source and the target language.']",Method_Citation
1,W06-3114,"Koehn and Monz, 2006",2,D07-1092,0,0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","[47, 9, 13]","['Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.', 'Training and testing is based on the Europarl corpus.', 'We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.']",Method_Citation
2,W06-3114,"Koehn and Monz, 2006",3,C08-1074,0,0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","[154, 46, 18]","['While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.', 'By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.', 'In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']",Method_Citation
3,W06-3114,"Koehn and Monz, 2006",4,W07-0718,0,0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","[90, 152, 108]","['Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.', 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', 'The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.']",Method_Citation
4,W06-3114,"Koehn and Monz, 2006",5,P07-1083,0,0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","[154, 113, 168]","['While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.', 'The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.', 'Annotators argued for the importance of having correct and even multiple references.']",Method_Citation
5,W06-3114,2006,6,W07-0738,0,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","[56, 163, 109]","['The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.', 'Not every annotator was fluent in both the source and the target language.', 'The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks), and then in graphical form in Figures 11–16.']",Method_Citation
6,W06-3114,2006,7,W07-0738,0,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","[56, 79, 168]","['The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.', 'Sentences and systems were randomly selected and randomly shuffled for presentation.', 'Annotators argued for the importance of having correct and even multiple references.']",Method_Citation
7,W06-3114,2006,8,W07-0738,0,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"[108, 168, 123]","['The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.', 'Annotators argued for the importance of having correct and even multiple references.', 'For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.']",Method_Citation
8,W06-3114,2006,9,W07-0738,0,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"[168, 56, 4]","['Annotators argued for the importance of having correct and even multiple references.', 'The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.', 'This revealed interesting clues about the properties of automatic and manual scoring.']",Method_Citation
9,W06-3114,"Koehn and Monz, 2006",10,D07-1030,0,0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","[52, 18, 49]","['Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.', 'In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.', 'Hence, we use the bootstrap resampling method described by Koehn (2004).']",Method_Citation
10,W06-3114,"Koehn and Monz, 2016",11,D07-1030,0,0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","[56, 123, 130]","['The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.', 'For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.', 'This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.']",Method_Citation
11,W06-3114,"Koehn and Monz, 2017",12,W08-0406,0,0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","[23, 117, 46]","['The domain is general politics, economics and science.', 'For instance: if 10 systems participate, and one system does better than 3 others, worse then 2, and is not significant different from the remaining 4, its rank is in the interval 3–7.', 'By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.']",Method_Citation
12,W06-3114,2006,13,W11-1002,0,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","[17, 168, 62]","['Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.', 'Annotators argued for the importance of having correct and even multiple references.', 'While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.']",Method_Citation
13,W06-3114,"Koehn and Monz, 2006",14,D07-1091,0,0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","[9, 24, 34]","['Training and testing is based on the Europarl corpus.', 'However, it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion.', 'For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']",Method_Citation
14,W06-3114,"Koehn and Monz, 2006",15,D07-1091,0,0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","[18, 13, 24]","['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.', 'We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.', 'However, it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion.']",Method_Citation
15,W06-3114,"Koehn and Monz, 2006",16,P07-1108,0,0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","[26, 33, 64]","['Most of these groups follow a phrase-based statistical approach to machine translation.', 'While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.', 'Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.']",Method_Citation
16,W06-3114,"Koehn and Monz, 2006",18,E12-3010,0,0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","[155, 79, 84]","['For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations.', 'Sentences and systems were randomly selected and randomly shuffled for presentation.', 'The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']",Method_Citation
17,W06-3114,"Koehn and Monz, 2006",19,W09-0402,0,0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","[51, 34, 24]","['When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.', 'For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.', 'However, it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion.']",Method_Citation
