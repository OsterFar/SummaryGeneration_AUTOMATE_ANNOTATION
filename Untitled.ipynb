{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "cb9708da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(invertedIndex,name) :\n",
    "  import json \n",
    "  with open('{0}.json'.format(name), 'w') as ij:\n",
    "    json.dump(invertedIndex,ij)\n",
    "\n",
    "  ij.close();\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911f668",
   "metadata": {},
   "source": [
    "# CITATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7ffe49f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def tokenization_caseFoldingg(FILE) :\n",
    "    # We're at the root node (<page>)\n",
    "    root_node = ET.parse('{0}.xml'.format(FILE)).getroot()\n",
    "\n",
    "    # We need to go one level below to get <items>\n",
    "    # and then one more level from that to go to <item>\n",
    "    dictt = {}\n",
    "    tokens = {}\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    #-------------------------------------------------------------------------\n",
    "    #                     A B S T R A C T \n",
    "    #-------------------------------------------------------------------------\n",
    "    for tag in root_node.findall('ABSTRACT/S'):\n",
    "        # Get the value from the attribute 'name'\n",
    "        counter += 1 \n",
    "        value = tag.attrib['sid']\n",
    "        #print(value)\n",
    "        # Get the text of that tag\n",
    "        string = tag.text\n",
    "        \n",
    "        dictt = invertedIndexx(string.lower(),i+1)\n",
    "        #myfile.close()\n",
    "        for x,y in dictt.items() :\n",
    "           if tokens.get(x) == None:\n",
    "                tokens.update({x:[]})\n",
    "           tokens[x].extend(y)\n",
    "        i += 1\n",
    "    #---------------------------------------------------------------------------\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    #                     S E C T I O N S\n",
    "    #-------------------------------------------------------------------------\n",
    "    for tag in root_node.findall('SECTION/S'):\n",
    "        # Get the value from the attribute 'name'\n",
    "        counter += 1 \n",
    "        value = tag.attrib['sid']\n",
    "        #print(value)\n",
    "        # Get the text of that tag\n",
    "        string = tag.text\n",
    "        \n",
    "        dictt = invertedIndexx(string.lower(),i+1)\n",
    "        #myfile.close()\n",
    "        for x,y in dictt.items() :\n",
    "           if tokens.get(x) == None:\n",
    "                tokens.update({x:[]})\n",
    "           tokens[x].extend(y)\n",
    "        i += 1\n",
    "    #---------------------------------------------------------------------------\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    return tokens , counter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f17f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e61cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ef4f45a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def StopWordsRemoval(token):\n",
    "    myfile = open('Stopword-List.txt',encoding='utf-8')\n",
    "    string = myfile.read()\n",
    "    dictt = {}\n",
    "    for x,y in token.items() :\n",
    "      for yy in y :\n",
    "        if yy not in string.split() :\n",
    "            if dictt.get(x) == None :\n",
    "                dictt.update({x:[]})\n",
    "            dictt[x].append(yy)\n",
    "    myfile.close()\n",
    "    return dictt\n",
    "\n",
    "def invertedIndexx(string,j): \n",
    "    s=''\n",
    "    tokens = {}\n",
    "    for i in range(len(string)) :\n",
    "        if string[i] != ' ' and string[i].isalnum():\n",
    "            s=s+string[i]\n",
    "        elif s!= '' :\n",
    "                \n",
    "            if tokens.get(j) == None :\n",
    "                    tokens.update({j:[]})\n",
    "            tokens[j].append(s.lower())\n",
    "            s=''\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization(token):\n",
    " \n",
    "  \n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  # newword = wordnet_lemmatizer.lemmatize(\"better\")\n",
    "  # print(newword)\n",
    "  temp = {}\n",
    "  for docid,words in token.items() :\n",
    "    for word in words :\n",
    "      newword = wordnet_lemmatizer.lemmatize(''.join(map(str, word)))\n",
    "      if temp.get(docid) == None :\n",
    "                  temp.update({docid:[]})\n",
    "      \n",
    "      temp[docid].append(newword)\n",
    "\n",
    "  \n",
    "\n",
    "  return temp\n",
    "\n",
    "def unique(list1,unique_list):\n",
    " \n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    # print list\n",
    "    \n",
    "    return unique_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b458d",
   "metadata": {},
   "source": [
    "## MAIN INDEX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c428518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortIndex(tokens , n) :\n",
    "  #print(n)\n",
    "  for i in range(n) :\n",
    "    \n",
    "    try :\n",
    "        tokens[i+1].sort()\n",
    "    except :\n",
    "        pass \n",
    "\n",
    "  return tokens\n",
    "\n",
    "import re #regular Expression \n",
    "\n",
    "def MainIndex(File) :\n",
    "  #breaking string into word and applying caseFolding \n",
    "  tokens , counter   = tokenization_caseFoldingg(File)\n",
    "  # print(len(tokens))\n",
    "  # print(tokens)\n",
    "  # #removingStopWords\n",
    "  # #tokens = StopWordsRemoval(tokens)\n",
    "  # print(len(tokens))\n",
    "  invertedIndex = tokens\n",
    "#   print(\"StopWordsRemoval->\")\n",
    "  invertedIndex = StopWordsRemoval(invertedIndex)\n",
    " # print(\"Okey\")\n",
    "#   print(\"lemmatization->\")\n",
    "  invertedIndex = lemmatization(invertedIndex)\n",
    "  #print(\"Okey\")\n",
    "  #invertedIndex = stemming(invertedIndex)\n",
    "  invertedIndex = sortIndex(invertedIndex ,  counter)\n",
    "  #print(invertedIndex)\n",
    "  return invertedIndex , counter \n",
    "  \n",
    "\n",
    "#Inverted_index , counter  = MainIndex()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488af089",
   "metadata": {},
   "source": [
    "## VSM CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "120f258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calTermFrequency(tokens,uniquelist , n) :\n",
    "  \n",
    "\n",
    "  vectorSpaceModel =  {}\n",
    "  \n",
    "  #InvertedIndex\n",
    "  for i in range(n) :\n",
    " \n",
    "    for Uword in uniquelist :\n",
    "      counter = 0 \n",
    "      try :\n",
    "            \n",
    "          counter = tokens[i+1].count(Uword)\n",
    "      except :\n",
    "        pass\n",
    "      #print(\"here==\",Uword,counter,i)\n",
    "      if vectorSpaceModel.get(i+1) == None :\n",
    "        vectorSpaceModel.update({(i+1):[]})\n",
    "      vectorSpaceModel[i+1].append(counter)\n",
    "  \n",
    "      \n",
    "  \n",
    "\n",
    "  return vectorSpaceModel\n",
    "\n",
    "def calTermFrequency_query(queryy,uniquelist , n) :\n",
    "\n",
    "  query_tf =  {}\n",
    "  \n",
    "  #InvertedIndex\n",
    "  \n",
    " \n",
    "  for Uword in uniquelist :\n",
    "    counter = 0 \n",
    "    counter = queryy.count(Uword)\n",
    "    # if counter >= 1 :\n",
    "    #   print(\"yes i am here \")\n",
    "    #print(\"here==\",Uword,counter,i)\n",
    "    if query_tf.get('query') == None :\n",
    "      query_tf.update({('query'):[]})\n",
    "    query_tf['query'].append(counter)\n",
    "  \n",
    "      \n",
    "  \n",
    "\n",
    "  return query_tf\n",
    "\n",
    "\"\"\"#### Document Frequency\"\"\"\n",
    "\n",
    "def DocumentFrequency(tokens,uniquelist ,cnt) :\n",
    "  df = []\n",
    "  for j in range(len(uniquelist)) :\n",
    "     counter = 0 \n",
    "     for i in range(cnt):\n",
    "       if tokens[i+1][j] != 0 :\n",
    "          counter = counter + 1\n",
    "      \n",
    "     df.append(counter)\n",
    "\n",
    "  try :\n",
    "      df = cal_idf(df,cnt)\n",
    "  except :\n",
    "      pass \n",
    "  return df\n",
    "\n",
    "\"\"\"#### TF IDF\"\"\"\n",
    "\n",
    "import math \n",
    "def cal_idf(df,n) :\n",
    "  for i in range(len(df)) :\n",
    "    idf = math.log(df[i],10) / n \n",
    "    #idf =  math.log(n/df[i],10)\n",
    "    df[i] = idf\n",
    "  return df\n",
    "\n",
    "def calculateTFIDF(vsm , uniquelt , df , counter) :\n",
    "\n",
    "  for i in range(len(uniquelt)) :\n",
    "    \n",
    "    for j in range(counter) :\n",
    "      vsm[j+1][i] = vsm[j+1][i] * df[i]\n",
    "\n",
    "  return vsm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6b181d18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def MainVSM(tokens,uniquelist,counter) :\n",
    "  \n",
    "  \n",
    "  #calculate the Unique list for VSM\n",
    "  for i in range(counter) :\n",
    "    try :\n",
    "        uniquelist = unique(tokens[i+1],uniquelist)\n",
    "    except :\n",
    "        pass \n",
    "  uniquelist.sort()\n",
    "#   print(uniquelist)\n",
    "  save(uniquelist,\"UniqueList\")\n",
    "  VSM  = calTermFrequency(tokens,uniquelist , counter)\n",
    "  save(VSM,\"TF\")\n",
    "  df = DocumentFrequency(VSM , uniquelist , counter)\n",
    "  VSM = calculateTFIDF(VSM,uniquelist,df,counter)\n",
    "#   print(VSM)\n",
    "  save(VSM,\"VSM\")\n",
    "  #print(len(uniquelist))\n",
    "\n",
    "  return VSM\n",
    "#cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1222e",
   "metadata": {},
   "source": [
    "\n",
    "# Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b37ce",
   "metadata": {},
   "source": [
    "## pre processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2593d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessingQuery(queryy) :\n",
    "  queryy = CaseFolding_query(queryy)\n",
    "  queryy = stopWordsRemoval_query(queryy)\n",
    "  queryy = lemmatization_query(queryy)\n",
    "  #queryy = stemming_query(queryy)\n",
    "  return queryy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "daa3910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CaseFolding_query(query) :\n",
    "  listt= []\n",
    "  for words in query :\n",
    "    listt.append(words.lower())\n",
    "  return listt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d18b1",
   "metadata": {},
   "source": [
    "## lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4cfb4d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization_query(query) :\n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  temp = []\n",
    "\n",
    "  for word in query :\n",
    "    newword = wordnet_lemmatizer.lemmatize(''.join(map(str, word)))\n",
    "    temp.append(newword)\n",
    "\n",
    "  return temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f70d6",
   "metadata": {},
   "source": [
    "## STop WorD QUERY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b6a1288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordsRemoval_query(queryy) :\n",
    "    myfile = open('Stopword-List.txt',encoding='utf-8')\n",
    "    string = myfile.read()\n",
    "    \n",
    "    #temp variable to store new words\n",
    "    temp = []\n",
    "    for word in queryy :\n",
    "      if word not in string.split() :\n",
    "        temp.append(word)\n",
    "    return temp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83015b0",
   "metadata": {},
   "source": [
    "## Stemming_query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "16ebe5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "def stemming_query(queryy) :\n",
    "  from nltk.stem import PorterStemmer\n",
    "  porter = PorterStemmer()\n",
    "\n",
    "  temp = []\n",
    "  for word in queryy :\n",
    "    newword = porter.stem(''.join(map(str, word)))\n",
    "    temp.append(newword) \n",
    "  return temp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725af3b8",
   "metadata": {},
   "source": [
    "## main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c973f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input Queries\n",
    "def Inputt(querystring) :\n",
    "    \n",
    "    query= Convert(querystring)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "59fe2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string to list converter\n",
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f9fe3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking input and storing in list\n",
    "\n",
    "\n",
    "def MAINQUERY(dataQuery) :\n",
    "    query = []\n",
    "    query = Inputt(dataQuery)\n",
    "\n",
    "    ##PreProcessing the Query \n",
    "    query = PreprocessingQuery(query)\n",
    "    #print(query)\n",
    "    global_query_TF = calTermFrequency_query(query,uniquelist,counter)\n",
    "    #print(len(global_query_TF['query']))\n",
    "    save(global_query_TF ,\"query\")\n",
    "    #print(type(global_query_TF))\n",
    "    return global_query_TF\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b5dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514ad615",
   "metadata": {},
   "source": [
    "\n",
    "# SIM CALCULATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378b42e",
   "metadata": {},
   "source": [
    "## Magnitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "234ef25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(values) :\n",
    "  mag = 0\n",
    "  for i in range(len(values)) :\n",
    "    mag = mag + math.pow(values[i], 2)\n",
    "  \n",
    "  mag = math.sqrt(mag)\n",
    "  return mag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692125d",
   "metadata": {},
   "source": [
    "## DOTPRODCUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9455d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProduct(doc,queryy) :\n",
    "  dotprod = 0\n",
    "  for i in range (len(doc)) :\n",
    "    dotprod = dotprod + doc[i]*queryy[i]\n",
    "\n",
    "  return dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "fa1f4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(uniquelist)\n",
    "save(uniquelist,\"uni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d99532",
   "metadata": {},
   "source": [
    "## SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "beeec0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def MainSIMVSM(global_query_TF) :\n",
    "    # now i have 2 global varible \n",
    "    # i) global_query_TF\n",
    "    # ii) global_VSM\n",
    "    COS_SIM = {}\n",
    "    #storing the magnitude of query as it will use in all doc\n",
    "    mag_query = magnitude(global_query_TF['query'])\n",
    "    #calculating for each document \n",
    "    for i in range(counter) :\n",
    "      numerator = DotProduct(global_VSM[str(i+1)],global_query_TF['query'])\n",
    "      mag_doc =  magnitude(global_VSM[str(i+1)])\n",
    "      denominator = mag_doc * mag_query \n",
    "      try :\n",
    "        ans = numerator / denominator\n",
    "      except :\n",
    "        ans = 0\n",
    "      COS_SIM.update({'SIMDOC{0}'.format(i+1):ans})\n",
    "\n",
    "\n",
    "    query_res = {}\n",
    "    #SIM\n",
    "    for i in range(len(COS_SIM)) :\n",
    "\n",
    "      val = COS_SIM['SIMDOC{0}'.format(i+1)]\n",
    "     # if val > 0.25:\n",
    "#         query_res.append(i+1)\n",
    "    \n",
    "      query_res.update({i+1 : val})\n",
    "\n",
    "    #print(query_res)\n",
    "    return query_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56483877",
   "metadata": {},
   "source": [
    "## CitationFinderRef(FileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "afff15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CitationFinderRef(FileName ,list_out):\n",
    "    \n",
    "    root_node = ET.parse('{0}.xml'.format(FileName)).getroot()\n",
    "    \n",
    "    # We need to go one level below to get <items>\n",
    "    # and then one more level from that to go to <item>\n",
    "    dictt = {}\n",
    "    tokens = {}\n",
    "    ret_value = []\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    #param = 90 \n",
    "    \n",
    "    for param in list_out :\n",
    "        #-------------------------------------------------------------------------\n",
    "        #                     A B S T R A C T \n",
    "        #-------------------------------------------------------------------------\n",
    "        for tag in root_node.findall('ABSTRACT/S'):\n",
    "            # Get the value from the attribute 'name'\n",
    "            counter += 1 \n",
    "            value = tag.attrib['sid']\n",
    "           #print(type(value))\n",
    "            if value == str(param) :\n",
    "                print(\"true\")\n",
    "                ret_value.append(tag.text)\n",
    "                break \n",
    "\n",
    "\n",
    "\n",
    "         #---------------------------------------------------------------------------\n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "        #                     S E C T I O N S\n",
    "        #-------------------------------------------------------------------------\n",
    "        for tag in root_node.findall('SECTION/S'):\n",
    "            ## Get the value from the attribute 'name'\n",
    "            counter += 1 \n",
    "            value = tag.attrib['sid']\n",
    "            #print(value)\n",
    "            if value == str(param) :\n",
    "                print(\"true\")\n",
    "                ret_value.append(tag.text)\n",
    "                break \n",
    "\n",
    "        #---------------------------------------------------------------------------\n",
    "\n",
    "    return ret_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3284c",
   "metadata": {},
   "source": [
    "## QUERY FROM CSV TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5866d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Section 5 compares our approach tooth ers in the literature, in particular that of (Miller et al., 2000)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major di erences: \u000f in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The approach in (Miller\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = One possibly bene cial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Similar to the approach in (Miller et al, 2000 )weinitialized the SLM statistics from the UPenn Tree bank parse trees (about 1Mwds of training data) at the rst training stage, see Section 3\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = (Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = WhereasMiller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = The syntactic model in (Miller et al, 2000) is similar to Collins?, but doesnot use features like sub cat frames and distance measures\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A00-2030/Reference_XML/A00-2030\n",
      "TEXT = Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = For our experiments, we use the NEGRA corpus (Skut et al, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training ,devel 1 10 100 1000 10000 100000 3 4 5 6 7 8 9 Frequenc y Parsing complexity head-driven optimal head-driven Figure 6: The distribution of parsing complexity among productions in Markovized, head-driven grammars read off from NEGRA-25\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = According to Skut et al (1997) tree banks have to meet the following requirements: 1\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997) .Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = Our data source is the German NeGra tree bank (Skut et al, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = German is con sider ably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA an notation has been conceived to be quite at (Skut et al, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = The factors used in the algorithms and the algorithms themselves are evaluated on a Germancorpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negratreebank (Skut et al, 1997) reports that lexicaliza tion of PCFGs decrease the parsing accuracy when parsing Negra? s flat constituent structures\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "A97-1014/Reference_XML/A97-1014\n",
      "TEXT = A comparison of unlexicalised PCFG parsing (Ku ?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = (Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = of English document and the second half of its aligned foreign language document (Mimno et al,2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D09-1092/Reference_XML/D09-1092\n",
      "TEXT = 3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Another popular task in SMT is domain adaptation (Foster et al, 2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010) .Research in Word Sense Disambiguation (Navigli, 2009, WSD), the task aimed at the automatic labeling of text with word senses, has been oriented towards domain text understanding for several years now\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = However, such confounding factors do not affect the optimization algorithm, which works with a fixed set of phrase pairs, and merely varies? .Our main technical contributions are as fol lows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Also, we independently perform perplexity minimization for all four features of the standard SMTtranslation model: the phrase translation probabilities p (t|s) and p (s|t), and the lexical weights lex (t|s) and lex (s|t)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) ex tend this approach by weighting individual phrase pairs\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al (2010) combine the two, applying linear interpolation to combine the instance 542 weighted out-of-domain model with an in-domain model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN) Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translationmodels.15 We demonstrate perplexity optimization for weighted counts, which are a natural extension of unadapted MLE training, but are of little prominence in domain adaptation research\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = m ?mpm (e? |f?) Our technique for setting? m is similar to that outlined in Foster et al (2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = m ?mpm (e? |f?) For efficiency and stability, we use the EMalgorithm to find??, rather than L-BFGS as in (Foster et al., 2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Foster et al (2010), however, uses a different approach to select related sentences from OUT\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Foster et al (2010) propose asimilar method for machine translation that uses features to capture degrees of generality\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = As in (Foster et al, 2010), this approach works at the level of phrase pairs\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and re port a decrease in performance\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = Foster et al (2010) further perform this on extracted phrase pairs, not just sentences\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "D10-1044/Reference_XML/D10-1044\n",
      "TEXT = To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Data-Oriented Parsing (DOP)? s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Goodman? s transform, in com bi nation with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Our best performing model is more accurate than all these previous models except (Bod, 2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance. Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current ac curacy level, and only 0.6% below the current best (Bod, 2003) .This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Similarly, (Bod, 2003) changes the way frequenciesfi are counted, with a similar effect\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = (henceforth, STSGs), which can represent single words, contiguous and noncontiguous MWEs, context-free rules or complete parse trees in a unified representation. My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Shown are results were only elementary trees with scores higher than 0.3 and 0.1 respectively are used. However, more interesting is a qualitative analysis of the STSG induced, which shows that ,un like DOP1, push-n-pull arrives at a grammar that gives high weights (and scores) to those elementary3We approximated the most probable parse as follows (fol lowing (Bod, 2003))\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = This result is only slightly higher than the highest reported result for this test-set, Bod? s (.907) (Bod,2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = Of course, it is well-known that a supervised parser? s f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f score if tested on the Brown corpus\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "E03-1005/Reference_XML/E03-1005\n",
      "TEXT = The probability of a parse tree T is the sum of the 1 This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "J01-2004/Reference_XML/J01-2004\n",
      "TEXT = At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = The method is described in (McCarthy et al, 2004), which we summarise here\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P04-1036/Reference_XML/P04-1036\n",
      "TEXT = The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = 1http: //sourceforge.net/projects/mstparser Figure 1: CoNLL-X dependency tree Figure 2: Penn Treebank-style phrase structure tree KSDEP Sagae and Tsujii (2007)? s dependencyparser,2 based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = For tree banks with non-projective trees weuse the pseudo-projective parsing technique to trans form the tree bank into projective structures (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson,2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = ,wn in O (n) time, producing a projective dependency graph satisfying conditions 1? 4 in section 2.1, possibly after adding arcs (0, i ,lr) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers) .Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to pre process training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods ,e.g., memory-based learning or support vector machines\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Whereas most of the early approaches were limited to strictly projective dependency structures, where the projection of a syntactic head must be continuous, attention has recently shifted to the analysis of non-projective structures, which are required for linguistically adequate representations, especially in languages with free or flexible word order. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Nova? k, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = troduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = non projective (Nivre and Nilsson, 2005), we char ac terise a sense in which the structures appearing in tree banks can be viewed as being only? slightly? ill-nested\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Weprojectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r? h, where r is the original label and h is the label of the original head in the non-projective dependency graph\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = Itshould be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in them selves (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P05-1013/Reference_XML/P05-1013\n",
      "TEXT = The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective trans formations as described in (Nivre and Nilsson, 2005)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = We use other WSM settings following Mitchell and Lapata (2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1028/Reference_XML/P08-1028\n",
      "TEXT = We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Goldberg and Tsarfaty (2008) pro pose a generative joint model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrewtext\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = 2The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = A study that is closely related toours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = 4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008) (Sec\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = It is the same grammar as described in (Goldberg and Tsarfaty, 2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1043/Reference_XML/P08-1043\n",
      "TEXT = Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parse val to use characters instead of space-delimited tokens as its basic units\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = plates called lexical-target in the column below areintroduced by Jiang et al (2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = We use the feature templates the same as Jiang et al, (2008) to extract features form E model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = approach, where basic processing units are characters which compose words (Jiangetal., 2008a)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = 6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = 6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P08-1102/Reference_XML/P08-1102\n",
      "TEXT = Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1060/Reference_XML/P11-1060\n",
      "TEXT = In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = (Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P11-1061/Reference_XML/P11-1061\n",
      "TEXT = We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formal ism, we refer to the literature. Let G be an LCFRS\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = There are many (structural) mildly context sensitive grammar formalisms ,e.g .mcfg ,lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al, 1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = We write REGD.k/ to refer to the class of regular dependency languages with a gap-degree bounded by k. Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006) .In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = LCFRS (Vijay-Shanker et al, 1987) are anatural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = A LCFRS (Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S )wherea) N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N; b) T and V are disjoint finite sets of terminals and variables; c) S? N is the start symbol with dim (S)= 1; d) P is a finite set of rewriting rules A (? 1,..\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987))\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = 2 By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of deriva tional structures are all regular (Vijay-Shanker et al., 1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975) .Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings ,i.e., discontinuous phrases\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "P87-1015/Reference_XML/P87-1015\n",
      "TEXT = CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991) .3 Thus, CFTG are mildly context-sensitive since their generated string languages are semi-linear and can be parsed in polynomial time (Go ?mez-Rodr? ?guez et al, 2010)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = Introduce through post-processing ,e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = Table 5 shows the official results for submitted parser outputs.31 The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = The high est score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (anda different policy regarding the inclusion of punctuation) .The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = McDonald et al (2006) use an additional algorithm\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)? s parser, (McDonald et al., 2006)? s parser, and so on\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = We have shown that, for languages with a7McDonald et al (2006) use post-processing for non projective dependencies and for labeling\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = 5It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = Entries marked with? are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-2932/Reference_XML/W06-2932\n",
      "TEXT = More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l)? Rk, where f is typically a bi nary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = We use the same method described in (Koehn and Monz, 2006) to perform the significance test\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W06-3114/Reference_XML/W06-3114\n",
      "TEXT = The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = (Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = (6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0613/Reference_XML/W99-0613\n",
      "TEXT = We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ \u000f|? f |+ L\u000f (1) where \u000f is a smoothing constant\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = 1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = 5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT = Henderson and Brill (1999) also reported that context did not help them to outperform simple voting\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = (Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = (Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "W99-0623/Reference_XML/W99-0623\n",
      "TEXT = Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined\n",
      "true\n",
      "true\n",
      "true\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "LIST_FOLDER = ['A00-2018','A00-2030','A97-1014','D09-1092','D10-1044','E03-1005','J01-2004','P04-1036','P05-1013','P08-1028','P08-1043','P08-1102','P11-1060','P11-1061','P87-1015','W06-2932','W06-3114','W99-0613','W99-0623']\n",
    "\n",
    "\n",
    "for Folder in LIST_FOLDER :\n",
    "    \n",
    "    \n",
    "    #============================================================================================================\n",
    "    #    new folder \n",
    "    df = pd.read_csv('{0}/annotation/{0}.csv'.format(Folder))\n",
    "\n",
    "    ############################################################################################\n",
    "    # Iterate loop to all rows by traversing all 3 col \n",
    "    LIST_OUTPUT = []\n",
    "    List_out_citation = []\n",
    "    list_out_fact = []\n",
    "    for i in df[['Reference Article','Citing Article','Citation Text']].itertuples():\n",
    "        #0 -> Index\n",
    "        #1 -> rA\n",
    "        #2 -> cA\n",
    "        #3 Citation Text \n",
    "\n",
    "        #making Indexes using Reference Document\n",
    "        FileName = i[1]\n",
    "        FileName = \"{0}/Reference_XML/{1}\".format(Folder,FileName)\n",
    "        print(FileName)\n",
    "        Inverted_index , counter  = MainIndex(FileName)\n",
    "\n",
    "        #MAKing INVERTED INDEX AND VSM CALCULATION \n",
    "\n",
    "        uniquelist = []\n",
    "        global_VSM = MainVSM(Inverted_index,uniquelist,counter)\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        #          S A V E \n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "        import json\n",
    "        with open(\"VSM.json\", 'r') as ii:\n",
    "            global_VSM = json.load(ii)\n",
    "\n",
    "\n",
    "        with open(\"UniqueList.json\", 'r') as ij:\n",
    "            uniquelist = json.load(ij)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "        #Making Query \n",
    "        global_query_TF = MAINQUERY(i[3])  # TEXT \n",
    "        print('TEXT =',i[3])\n",
    "\n",
    "        Result = MainSIMVSM(global_query_TF)\n",
    "\n",
    "        #SORTING dictionARY----------\n",
    "\n",
    "        ANS = {k: v for k, v in sorted(Result.items(), key=lambda item: item[1])}\n",
    "\n",
    "        #REVERSE \n",
    "        res = OrderedDict(reversed(list(ANS.items())))\n",
    "        Output = []\n",
    "        counter = 0\n",
    "        for i in res :\n",
    "            if counter > 2 :\n",
    "                break\n",
    "            counter = counter + 1\n",
    "            Output.append(i)\n",
    "\n",
    "        #Storing the values in list\n",
    "        LIST_OUTPUT.append(Output)\n",
    "\n",
    "        #calling citaitonFinder function and storing value in list in order to make dataframe in future\n",
    "        List_out_citation.append( CitationFinderRef(FileName , Output) )\n",
    "\n",
    "        #3rd Output \n",
    "        list_out_fact.append(\"Method_Citation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #---------------------------\n",
    "        #print(df)\n",
    "        #print(Result)\n",
    "        print(\"\\n \\n\")\n",
    "        ANS = {}\n",
    "        Result = {}\n",
    "        global_query_TF= {}\n",
    "    \n",
    "    #=====================================================================================\n",
    "\n",
    "\n",
    "\n",
    "    # making the new data frame \n",
    "    dictt = {\n",
    "        'Reference Article' :df['Reference Article'] ,\n",
    "        'Citation Marker' : df[\"Citation Marker\"] ,\n",
    "        'Citance Number' : df[\"Citance Number\"] ,\n",
    "        'Citing Article' : df[\"Citing Article\"] ,\n",
    "        'Citation Marker Offset' : df[\"Citation Marker Offset\"] ,\n",
    "        'Citation Offset' : df[\"Citation Offset\"] ,\n",
    "        'Citation Text' : df[\"Citation Text\"] ,\n",
    "        'Reference Offset' : LIST_OUTPUT ,\n",
    "        'Reference Textt' : List_out_citation ,\n",
    "        'Discourse Facet' : list_out_fact \n",
    "    }\n",
    "    NewDF  = pd.DataFrame(dictt)\n",
    "\n",
    "    NewDF.to_csv('{0}/annotation/AnnotationNew.csv'.format(Folder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f346197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4eb7f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFAULT\n",
    "\n",
    "Inverted_index , counter  = MainIndex(FileName)\n",
    "\n",
    "\n",
    "uniquelist = []\n",
    "global_VSM = MainVSM(Inverted_index,uniquelist,counter)\n",
    "\n",
    "#saving \n",
    "#------------------------------------------------------\n",
    "import json\n",
    "with open(\"VSM.json\", 'r') as ii:\n",
    "    global_VSM = json.load(ii)\n",
    "\n",
    "\n",
    "with open(\"UniqueList.json\", 'r') as ij:\n",
    "    uniquelist = json.load(ij)\n",
    "#------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24750e14",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ade129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1492cd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('A00-2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3bdf382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # print(\"---------------------------\")\n",
    "# # i[0] = 2\n",
    "# # a = i[0]\n",
    "# # print( df.loc[a,'Reference Article'] )\n",
    "# # df.loc[2,'Reference Article'] = 10\n",
    "# # print( df.loc[a,'Reference Article'] )\n",
    "# # print( df.loc[3,'Reference Article'] )\n",
    "# dictt = {\n",
    "#     'Reference Article' :df['Reference Article'] ,\n",
    "#     'Citation Marker' : df[\"Citation Marker\"]\n",
    "# }\n",
    "# print(dictt[\"Citation Marker\"])\n",
    "# ii  = pd.DataFrame(dictt)\n",
    "# print(ii)\n",
    "# # df[\"itance Number\"][2] = 500\n",
    "# # print( df[\"itance Number\"][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd4a06",
   "metadata": {},
   "source": [
    "## PRE PROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443929aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e4b62f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308cab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
