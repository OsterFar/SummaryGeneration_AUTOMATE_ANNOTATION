{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9708da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(invertedIndex,name) :\n",
    "  import json \n",
    "  with open('{0}.json'.format(name), 'w') as ij:\n",
    "    json.dump(invertedIndex,ij)\n",
    "\n",
    "  ij.close();\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a911f668",
   "metadata": {},
   "source": [
    "# CITATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffe49f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def tokenization_caseFoldingg(FILE) :\n",
    "    # We're at the root node (<page>)\n",
    "    root_node = ET.parse('{0}.xml'.format(FILE)).getroot()\n",
    "\n",
    "    # We need to go one level below to get <items>\n",
    "    # and then one more level from that to go to <item>\n",
    "    dictt = {}\n",
    "    tokens = {}\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    #-------------------------------------------------------------------------\n",
    "    #                     A B S T R A C T \n",
    "    #-------------------------------------------------------------------------\n",
    "    for tag in root_node.findall('ABSTRACT/S'):\n",
    "        # Get the value from the attribute 'name'\n",
    "        counter += 1 \n",
    "        value = tag.attrib['sid']\n",
    "        #print(value)\n",
    "        # Get the text of that tag\n",
    "        string = tag.text\n",
    "        \n",
    "        dictt = invertedIndexx(string.lower(),i+1)\n",
    "        #myfile.close()\n",
    "        for x,y in dictt.items() :\n",
    "           if tokens.get(x) == None:\n",
    "                tokens.update({x:[]})\n",
    "           tokens[x].extend(y)\n",
    "        i += 1\n",
    "    #---------------------------------------------------------------------------\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    #                     S E C T I O N S\n",
    "    #-------------------------------------------------------------------------\n",
    "    for tag in root_node.findall('SECTION/S'):\n",
    "        # Get the value from the attribute 'name'\n",
    "        counter += 1 \n",
    "        value = tag.attrib['sid']\n",
    "        #print(value)\n",
    "        # Get the text of that tag\n",
    "        string = tag.text\n",
    "        \n",
    "        dictt = invertedIndexx(string.lower(),i+1)\n",
    "        #myfile.close()\n",
    "        for x,y in dictt.items() :\n",
    "           if tokens.get(x) == None:\n",
    "                tokens.update({x:[]})\n",
    "           tokens[x].extend(y)\n",
    "        i += 1\n",
    "    #---------------------------------------------------------------------------\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    return tokens , counter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f17f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e61cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef4f45a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def StopWordsRemoval(token):\n",
    "    myfile = open('Stopword-List.txt',encoding='utf-8')\n",
    "    string = myfile.read()\n",
    "    dictt = {}\n",
    "    for x,y in token.items() :\n",
    "      for yy in y :\n",
    "        if yy not in string.split() :\n",
    "            if dictt.get(x) == None :\n",
    "                dictt.update({x:[]})\n",
    "            dictt[x].append(yy)\n",
    "    myfile.close()\n",
    "    return dictt\n",
    "\n",
    "def invertedIndexx(string,j): \n",
    "    s=''\n",
    "    tokens = {}\n",
    "    for i in range(len(string)) :\n",
    "        if string[i] != ' ' and string[i].isalnum():\n",
    "            s=s+string[i]\n",
    "        elif s!= '' :\n",
    "                \n",
    "            if tokens.get(j) == None :\n",
    "                    tokens.update({j:[]})\n",
    "            tokens[j].append(s.lower())\n",
    "            s=''\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization(token):\n",
    " \n",
    "  \n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  # newword = wordnet_lemmatizer.lemmatize(\"better\")\n",
    "  # print(newword)\n",
    "  temp = {}\n",
    "  for docid,words in token.items() :\n",
    "    for word in words :\n",
    "      newword = wordnet_lemmatizer.lemmatize(''.join(map(str, word)))\n",
    "      if temp.get(docid) == None :\n",
    "                  temp.update({docid:[]})\n",
    "      \n",
    "      temp[docid].append(newword)\n",
    "\n",
    "  \n",
    "\n",
    "  return temp\n",
    "\n",
    "def unique(list1,unique_list):\n",
    " \n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    # print list\n",
    "    \n",
    "    return unique_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b458d",
   "metadata": {},
   "source": [
    "## MAIN INDEX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c428518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortIndex(tokens , n) :\n",
    "  #print(n)\n",
    "  for i in range(n) :\n",
    "    \n",
    "    try :\n",
    "        tokens[i+1].sort()\n",
    "    except :\n",
    "        pass \n",
    "\n",
    "  return tokens\n",
    "\n",
    "import re #regular Expression \n",
    "\n",
    "def MainIndex(File) :\n",
    "  #breaking string into word and applying caseFolding \n",
    "  tokens , counter   = tokenization_caseFoldingg(File)\n",
    "  # print(len(tokens))\n",
    "  # print(tokens)\n",
    "  # #removingStopWords\n",
    "  # #tokens = StopWordsRemoval(tokens)\n",
    "  # print(len(tokens))\n",
    "  invertedIndex = tokens\n",
    "#   print(\"StopWordsRemoval->\")\n",
    "  invertedIndex = StopWordsRemoval(invertedIndex)\n",
    " # print(\"Okey\")\n",
    "#   print(\"lemmatization->\")\n",
    "  invertedIndex = lemmatization(invertedIndex)\n",
    "  #print(\"Okey\")\n",
    "  #invertedIndex = stemming(invertedIndex)\n",
    "  invertedIndex = sortIndex(invertedIndex ,  counter)\n",
    "  #print(invertedIndex)\n",
    "  return invertedIndex , counter \n",
    "  \n",
    "\n",
    "#Inverted_index , counter  = MainIndex()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488af089",
   "metadata": {},
   "source": [
    "## VSM CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "120f258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calTermFrequency(tokens,uniquelist , n) :\n",
    "  \n",
    "\n",
    "  vectorSpaceModel =  {}\n",
    "  \n",
    "  #InvertedIndex\n",
    "  for i in range(n) :\n",
    " \n",
    "    for Uword in uniquelist :\n",
    "      counter = 0 \n",
    "      try :\n",
    "            \n",
    "          counter = tokens[i+1].count(Uword)\n",
    "      except :\n",
    "        pass\n",
    "      #print(\"here==\",Uword,counter,i)\n",
    "      if vectorSpaceModel.get(i+1) == None :\n",
    "        vectorSpaceModel.update({(i+1):[]})\n",
    "      vectorSpaceModel[i+1].append(counter)\n",
    "  \n",
    "      \n",
    "  \n",
    "\n",
    "  return vectorSpaceModel\n",
    "\n",
    "def calTermFrequency_query(queryy,uniquelist , n) :\n",
    "\n",
    "  query_tf =  {}\n",
    "  \n",
    "  #InvertedIndex\n",
    "  \n",
    " \n",
    "  for Uword in uniquelist :\n",
    "    counter = 0 \n",
    "    counter = queryy.count(Uword)\n",
    "    # if counter >= 1 :\n",
    "    #   print(\"yes i am here \")\n",
    "    #print(\"here==\",Uword,counter,i)\n",
    "    if query_tf.get('query') == None :\n",
    "      query_tf.update({('query'):[]})\n",
    "    query_tf['query'].append(counter)\n",
    "  \n",
    "      \n",
    "  \n",
    "\n",
    "  return query_tf\n",
    "\n",
    "\"\"\"#### Document Frequency\"\"\"\n",
    "\n",
    "def DocumentFrequency(tokens,uniquelist ,cnt) :\n",
    "  df = []\n",
    "  for j in range(len(uniquelist)) :\n",
    "     counter = 0 \n",
    "     for i in range(cnt):\n",
    "       if tokens[i+1][j] != 0 :\n",
    "          counter = counter + 1\n",
    "      \n",
    "     df.append(counter)\n",
    "\n",
    "  try :\n",
    "      df = cal_idf(df,cnt)\n",
    "  except :\n",
    "      pass \n",
    "  return df\n",
    "\n",
    "\"\"\"#### TF IDF\"\"\"\n",
    "\n",
    "import math \n",
    "def cal_idf(df,n) :\n",
    "  for i in range(len(df)) :\n",
    "    idf = math.log(df[i],10) / n \n",
    "    #idf =  math.log(n/df[i],10)\n",
    "    df[i] = idf\n",
    "  return df\n",
    "\n",
    "def calculateTFIDF(vsm , uniquelt , df , counter) :\n",
    "\n",
    "  for i in range(len(uniquelt)) :\n",
    "    \n",
    "    for j in range(counter) :\n",
    "      vsm[j+1][i] = vsm[j+1][i] * df[i]\n",
    "\n",
    "  return vsm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b181d18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def MainVSM(tokens,uniquelist,counter) :\n",
    "  \n",
    "  \n",
    "  #calculate the Unique list for VSM\n",
    "  for i in range(counter) :\n",
    "    try :\n",
    "        uniquelist = unique(tokens[i+1],uniquelist)\n",
    "    except :\n",
    "        pass \n",
    "  uniquelist.sort()\n",
    "#   print(uniquelist)\n",
    "  save(uniquelist,\"UniqueList\")\n",
    "  VSM  = calTermFrequency(tokens,uniquelist , counter)\n",
    "  save(VSM,\"TF\")\n",
    "  df = DocumentFrequency(VSM , uniquelist , counter)\n",
    "  VSM = calculateTFIDF(VSM,uniquelist,df,counter)\n",
    "#   print(VSM)\n",
    "  save(VSM,\"VSM\")\n",
    "  #print(len(uniquelist))\n",
    "\n",
    "  return VSM\n",
    "#cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b1222e",
   "metadata": {},
   "source": [
    "\n",
    "# Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b37ce",
   "metadata": {},
   "source": [
    "## pre processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2593d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessingQuery(queryy) :\n",
    "  queryy = CaseFolding_query(queryy)\n",
    "  queryy = stopWordsRemoval_query(queryy)\n",
    "  queryy = lemmatization_query(queryy)\n",
    "  #queryy = stemming_query(queryy)\n",
    "  return queryy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daa3910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CaseFolding_query(query) :\n",
    "  listt= []\n",
    "  for words in query :\n",
    "    listt.append(words.lower())\n",
    "  return listt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d18b1",
   "metadata": {},
   "source": [
    "## lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cfb4d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatization_query(query) :\n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  temp = []\n",
    "\n",
    "  for word in query :\n",
    "    newword = wordnet_lemmatizer.lemmatize(''.join(map(str, word)))\n",
    "    temp.append(newword)\n",
    "\n",
    "  return temp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f70d6",
   "metadata": {},
   "source": [
    "## STop WorD QUERY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a1288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordsRemoval_query(queryy) :\n",
    "    myfile = open('Stopword-List.txt',encoding='utf-8')\n",
    "    string = myfile.read()\n",
    "    \n",
    "    #temp variable to store new words\n",
    "    temp = []\n",
    "    for word in queryy :\n",
    "      if word not in string.split() :\n",
    "        temp.append(word)\n",
    "    return temp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83015b0",
   "metadata": {},
   "source": [
    "## Stemming_query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16ebe5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "def stemming_query(queryy) :\n",
    "  from nltk.stem import PorterStemmer\n",
    "  porter = PorterStemmer()\n",
    "\n",
    "  temp = []\n",
    "  for word in queryy :\n",
    "    newword = porter.stem(''.join(map(str, word)))\n",
    "    temp.append(newword) \n",
    "  return temp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725af3b8",
   "metadata": {},
   "source": [
    "## main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c973f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input Queries\n",
    "def Inputt(querystring) :\n",
    "    \n",
    "    query= Convert(querystring)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59fe2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string to list converter\n",
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9fe3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking input and storing in list\n",
    "\n",
    "\n",
    "def MAINQUERY(dataQuery) :\n",
    "    query = []\n",
    "    query = Inputt(dataQuery)\n",
    "\n",
    "    ##PreProcessing the Query \n",
    "    query = PreprocessingQuery(query)\n",
    "    #print(query)\n",
    "    global_query_TF = calTermFrequency_query(query,uniquelist,counter)\n",
    "    #print(len(global_query_TF['query']))\n",
    "    save(global_query_TF ,\"query\")\n",
    "    #print(type(global_query_TF))\n",
    "    return global_query_TF\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b5dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "514ad615",
   "metadata": {},
   "source": [
    "\n",
    "# SIM CALCULATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378b42e",
   "metadata": {},
   "source": [
    "## Magnitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "234ef25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude(values) :\n",
    "  mag = 0\n",
    "  for i in range(len(values)) :\n",
    "    mag = mag + math.pow(values[i], 2)\n",
    "  \n",
    "  mag = math.sqrt(mag)\n",
    "  return mag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692125d",
   "metadata": {},
   "source": [
    "## DOTPRODCUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9455d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProduct(doc,queryy) :\n",
    "  dotprod = 0\n",
    "  for i in range (len(doc)) :\n",
    "    dotprod = dotprod + doc[i]*queryy[i]\n",
    "\n",
    "  return dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa1f4f00",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniquelist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e6f261e7cc37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(uniquelist)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniquelist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"uni\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'uniquelist' is not defined"
     ]
    }
   ],
   "source": [
    "#print(uniquelist)\n",
    "save(uniquelist,\"uni\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d99532",
   "metadata": {},
   "source": [
    "## SIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beeec0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def MainSIMVSM(global_query_TF) :\n",
    "    # now i have 2 global varible \n",
    "    # i) global_query_TF\n",
    "    # ii) global_VSM\n",
    "    COS_SIM = {}\n",
    "    #storing the magnitude of query as it will use in all doc\n",
    "    mag_query = magnitude(global_query_TF['query'])\n",
    "    #calculating for each document \n",
    "    for i in range(counter) :\n",
    "      numerator = DotProduct(global_VSM[str(i+1)],global_query_TF['query'])\n",
    "      mag_doc =  magnitude(global_VSM[str(i+1)])\n",
    "      denominator = mag_doc * mag_query \n",
    "      try :\n",
    "        ans = numerator / denominator\n",
    "      except :\n",
    "        ans = 0\n",
    "      COS_SIM.update({'SIMDOC{0}'.format(i+1):ans})\n",
    "\n",
    "\n",
    "    query_res = {}\n",
    "    #SIM\n",
    "    for i in range(len(COS_SIM)) :\n",
    "\n",
    "      val = COS_SIM['SIMDOC{0}'.format(i+1)]\n",
    "     # if val > 0.25:\n",
    "#         query_res.append(i+1)\n",
    "    \n",
    "      query_res.update({i+1 : val})\n",
    "\n",
    "    #print(query_res)\n",
    "    return query_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56483877",
   "metadata": {},
   "source": [
    "## CitationFinderRef(FileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afff15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CitationFinderRef(FileName ,list_out):\n",
    "    \n",
    "    root_node = ET.parse('{0}.xml'.format(FileName)).getroot()\n",
    "    \n",
    "    # We need to go one level below to get <items>\n",
    "    # and then one more level from that to go to <item>\n",
    "    dictt = {}\n",
    "    tokens = {}\n",
    "    ret_value = \"\"\n",
    "    counter = 0\n",
    "    i = 0\n",
    "    #param = 90 \n",
    "    \n",
    "    for param in list_out :\n",
    "        #-------------------------------------------------------------------------\n",
    "        #                     A B S T R A C T \n",
    "        #-------------------------------------------------------------------------\n",
    "        for tag in root_node.findall('ABSTRACT/S'):\n",
    "            # Get the value from the attribute 'name'\n",
    "            counter += 1 \n",
    "            value = tag.attrib['sid']\n",
    "           #print(type(value))\n",
    "            if value == str(param) :\n",
    "                print(\"true\")\n",
    "                ret_value = tag.text\n",
    "                break \n",
    "\n",
    "\n",
    "\n",
    "         #---------------------------------------------------------------------------\n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "        #                     S E C T I O N S\n",
    "        #-------------------------------------------------------------------------\n",
    "        for tag in root_node.findall('SECTION/S'):\n",
    "            ## Get the value from the attribute 'name'\n",
    "            counter += 1 \n",
    "            value = tag.attrib['sid']\n",
    "            #print(value)\n",
    "            if value == str(param) :\n",
    "                print(\"true\")\n",
    "                ret_value = tag.text\n",
    "                break \n",
    "\n",
    "        #---------------------------------------------------------------------------\n",
    "\n",
    "    return ret_value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3284c",
   "metadata": {},
   "source": [
    "## QUERY FROM CSV TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "443929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def FileGeneration(list_directories) :\n",
    "    listt = []\n",
    "    directory = r'C:\\Users\\LEO\\Untitled Folder\\DataSet'\n",
    "    # for filename in os.listdir(directory):\n",
    "    #     listt.append(filename)\n",
    "    list_directories = [f for f in os.listdir(directory)]    \n",
    "    return list_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5866d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions\n",
      "true\n",
      "\n",
      " \n",
      "\n",
      "DataSet/A00-2018/Reference_XML/A00-2018\n",
      "TEXT = Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size\n",
      "true\n",
      "\n",
      " \n",
      "\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'DataSet/A00-2018/annotation/AnnotationNew.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-e9a96eaef900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mNewDF\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mNewDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataSet/{0}/annotation/AnnotationNew.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'DataSet/A00-2018/annotation/AnnotationNew.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "#IST_FOLDER = ['A00-2018','A00-2030','A97-1014','D09-1092','D10-1044','E03-1005','J01-2004','P04-1036','P05-1013','P08-1028','P08-1043','P08-1102','P11-1060','P11-1061','P87-1015','W06-2932','W06-3114','W99-0613','W99-0623']\n",
    "LIST_FOLDER = []\n",
    "LIST_FOLDER = FileGeneration(LIST_FOLDER)\n",
    "\n",
    "for Folder in LIST_FOLDER :\n",
    "    \n",
    "    \n",
    "    #============================================================================================================\n",
    "    #    new folder \n",
    "    df = pd.read_csv('DataSet/{0}/annotation/{0}.csv'.format(Folder))\n",
    "\n",
    "    ############################################################################################\n",
    "    # Iterate loop to all rows by traversing all 3 col \n",
    "    LIST_OUTPUT = []\n",
    "    List_out_citation = []\n",
    "    list_out_fact = []\n",
    "    for i in df[['Reference Article','Citing Article','Citation Text']].itertuples():\n",
    "        #0 -> Index\n",
    "        #1 -> rA\n",
    "        #2 -> cA\n",
    "        #3 Citation Text \n",
    "\n",
    "        #making Indexes using Reference Document\n",
    "        FileName = i[1]\n",
    "        FileName = \"DataSet/{0}/Reference_XML/{1}\".format(Folder,FileName)\n",
    "        print(FileName)\n",
    "        Inverted_index , counter  = MainIndex(FileName)\n",
    "\n",
    "        #MAKing INVERTED INDEX AND VSM CALCULATION \n",
    "\n",
    "        uniquelist = []\n",
    "        global_VSM = MainVSM(Inverted_index,uniquelist,counter)\n",
    "\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        #          S A V E \n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "        import json\n",
    "        with open(\"VSM.json\", 'r') as ii:\n",
    "            global_VSM = json.load(ii)\n",
    "\n",
    "\n",
    "        with open(\"UniqueList.json\", 'r') as ij:\n",
    "            uniquelist = json.load(ij)\n",
    "\n",
    "        #-----------------------------------------------------------------------\n",
    "        #-----------------------------------------------------------------------\n",
    "\n",
    "        #Making Query \n",
    "        global_query_TF = MAINQUERY(i[3])  # TEXT \n",
    "        print('TEXT =',i[3])\n",
    "\n",
    "        Result = MainSIMVSM(global_query_TF)\n",
    "\n",
    "        #SORTING dictionARY----------\n",
    "\n",
    "        ANS = {k: v for k, v in sorted(Result.items(), key=lambda item: item[1])}\n",
    "\n",
    "        #REVERSE \n",
    "        res = OrderedDict(reversed(list(ANS.items())))\n",
    "        Output = []\n",
    "        counter = 0\n",
    "        for i in res :\n",
    "            if counter > 0 :\n",
    "                break\n",
    "            counter = counter + 1\n",
    "            Output.append(i)\n",
    "\n",
    "        #Storing the values in list\n",
    "        LIST_OUTPUT.append(Output)\n",
    "\n",
    "        #calling citaitonFinder function and storing value in list in order to make dataframe in future\n",
    "        List_out_citation.append( CitationFinderRef(FileName , Output) )\n",
    "\n",
    "        #3rd Output \n",
    "        list_out_fact.append(\"Method_Citation\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #---------------------------\n",
    "        #print(df)\n",
    "        #print(Result)\n",
    "        print(\"\\n \\n\")\n",
    "        ANS = {}\n",
    "        Result = {}\n",
    "        global_query_TF= {}\n",
    "    \n",
    "    #=====================================================================================\n",
    "\n",
    "\n",
    "\n",
    "    # making the new data frame \n",
    "    dictt = {\n",
    "        'Reference Article' :df['Reference Article'] ,\n",
    "        'Citation Marker' : df[\"Citation Marker\"] ,\n",
    "        'Citance Number' : df[\"Citance Number\"] ,\n",
    "        'Citing Article' : df[\"Citing Article\"] ,\n",
    "        'Citation Marker Offset' : df[\"Citation Marker Offset\"] ,\n",
    "        'Citation Offset' : df[\"Citation Offset\"] ,\n",
    "        'Citation Text' : df[\"Citation Text\"] ,\n",
    "        'Reference Offset' : LIST_OUTPUT ,\n",
    "        'Reference Textt' : List_out_citation ,\n",
    "        'Discourse Facet' : list_out_fact \n",
    "    }\n",
    "    NewDF  = pd.DataFrame(dictt)\n",
    "\n",
    "    NewDF.to_csv('DataSet/{0}/annotation/AnnotationNew.csv'.format(Folder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f346197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24750e14",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4eb7f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEFAULT\n",
    "\n",
    "# Inverted_index , counter  = MainIndex(FileName)\n",
    "\n",
    "\n",
    "# uniquelist = []\n",
    "# global_VSM = MainVSM(Inverted_index,uniquelist,counter)\n",
    "\n",
    "# #saving \n",
    "# #------------------------------------------------------\n",
    "# import json\n",
    "# with open(\"VSM.json\", 'r') as ii:\n",
    "#     global_VSM = json.load(ii)\n",
    "\n",
    "\n",
    "# with open(\"UniqueList.json\", 'r') as ij:\n",
    "#     uniquelist = json.load(ij)\n",
    "# #------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "ade129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1492cd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('A00-2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3bdf382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # print(\"---------------------------\")\n",
    "# # i[0] = 2\n",
    "# # a = i[0]\n",
    "# # print( df.loc[a,'Reference Article'] )\n",
    "# # df.loc[2,'Reference Article'] = 10\n",
    "# # print( df.loc[a,'Reference Article'] )\n",
    "# # print( df.loc[3,'Reference Article'] )\n",
    "# dictt = {\n",
    "#     'Reference Article' :df['Reference Article'] ,\n",
    "#     'Citation Marker' : df[\"Citation Marker\"]\n",
    "# }\n",
    "# print(dictt[\"Citation Marker\"])\n",
    "# ii  = pd.DataFrame(dictt)\n",
    "# print(ii)\n",
    "# # df[\"itance Number\"][2] = 500\n",
    "# # print( df[\"itance Number\"][2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd4a06",
   "metadata": {},
   "source": [
    "## PRE PROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b62f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2e086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
