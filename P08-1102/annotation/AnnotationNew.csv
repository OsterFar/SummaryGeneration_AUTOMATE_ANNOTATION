,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,P08-1102,2008,1,C08-1049,0,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","[1, 10, 32]","['We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', 'Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).', 'We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.']",Method_Citation
1,P08-1102,2008,2,C08-1049,0,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","[10, 114, 25]","['Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).', 'We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing.', 'According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.']",Method_Citation
2,P08-1102,2008,3,C08-1049,0,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),"[40, 7, 107]","['Templates in the column below are expanded from the upper ones.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks.', 'The evaluation results are shown in Table 3.']",Method_Citation
3,P08-1102,2008,4,P12-1110,0,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","[54, 104, 71]","['We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones, not to mention the higher-order grams such as trigrams or 4-grams.', 'According to the usual practice in syntactic analysis, we choose chapters 1 − 260 (18074 sentences) as training set, chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.', 'Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: λwt and λtw denote the corresponding weights of the two components.']",Method_Citation
4,P08-1102,2008,5,D12-1126,0,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"[1, 130, 9]","['We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', 'We proposed a cascaded linear model for Chinese Joint S&T.', 'To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).']",Method_Citation
5,P08-1102,2008,6,C10-1135,0,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","[54, 116, 63]","['We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones, not to mention the higher-order grams such as trigrams or 4-grams.', 'In order to inspect how much improvement each feature brings into the cascaded model, every time we removed a feature while retaining others, then retrained the model and tested its performance on the test set.', 'As all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.']",Method_Citation
6,P08-1102,"Jiangetal., 2008a",8,P12-1025,0,0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","[107, 36, 51]","['The evaluation results are shown in Table 3.', 'All feature templates and their instances are shown in Table 1.', 'Additional features most widely used are related to word or POS ngrams.']",Method_Citation
7,P08-1102,2008b,9,C10-2096,0,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","[66, 77, 40]","['Besides the output of the perceptron, the outside-layer also receive the outputs of the word LM, the POS LM, the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.', 'In Chinese Joint S&T, the mission of the decoder is to find the boundary-POS labelled sequence with the highest score.', 'Templates in the column below are expanded from the upper ones.']",Method_Citation
8,P08-1102,"Jiang et al, 2008a",10,C10-2096,0,0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","[26, 35, 46]","['In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).', 'To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.', 'Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.']",Method_Citation
9,P08-1102,"Jiang et al, 2008a",11,C10-2096,0,0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","[81, 96, 66]","['When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.', 'In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.', 'Besides the output of the perceptron, the outside-layer also receive the outputs of the word LM, the POS LM, the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.']",Method_Citation
10,P08-1102,"Jiang et al, 2008",12,C10-1132,0,0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","[40, 137, 7]","['Templates in the column below are expanded from the upper ones.', 'We will investigate these problems in the following work.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks.']",Method_Citation
11,P08-1102,"Jiang et al, 2008",13,C10-1132,0,0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","[38, 95, 40]","['Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.', 'For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus.', 'Templates in the column below are expanded from the upper ones.']",Method_Citation
12,P08-1102,"Jiang et al, 2008",14,C10-1132,0,0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","[61, 78, 16]","['In this layer, each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.', 'Given a Chinese character sequence C1:n, the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.', 'Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.']",Method_Citation
13,P08-1102,"Jiang et al, 2008",15,C10-1132,0,0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","[44, 82, 62]","['With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.', 'In addition, we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.', 'Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.']",Method_Citation
14,P08-1102,"Jiang et al, 2008",17,C10-1132,0,0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","[31, 110, 21]","['The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.', 'Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation.', 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.']",Method_Citation
15,P08-1102,Jiang et al2008a,20,D12-1046,0,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","[31, 3, 1]","['The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.', 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', 'We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.']",Method_Citation
