,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,J01-2004,2001,1,W05-0104,0,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","[106, 16, 316]","['String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used in language models.', 'While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task.', 'In order to have a proper probability distribution, we would need to renormalize by dividing by some factor.']",Method_Citation
1,J01-2004,2001,2,P08-1013,0,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)","[115, 404, 322]","['The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.', 'Also, the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram, and lead to better off-line language models than those that we have presented here.', 'Thus, Chelba and Jelinek (1998a, 1998b) also used a parser to help assign word probabilities, via the structured language model outlined in Section 3.2.']",Method_Citation
2,J01-2004,"Roark, 2001a",4,P04-1015,0,0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","[367, 347, 248]","['Table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40M words, with a vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was interpolated with the lattice trigram at A -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.', 'The perplexity improvement was achieved by simply taking the existing parsing model and applying it, with no extra training beyond that done for parsing.', 'In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.']",Method_Citation
3,J01-2004,"Roark, 2001a",5,P04-1015,0,0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","[367, 348, 248]","['Table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40M words, with a vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was interpolated with the lattice trigram at A -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.', 'The hope was expressed above that our reported perplexity would be fairly close to the &quot;true&quot; perplexity that we would achieve if the model were properly normalized, i.e., that the amount of probability mass that we lose by pruning is small.', 'In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.']",Method_Citation
4,J01-2004,2001a,6,P04-1015,0,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","[147, 178, 414]","['One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).', 'If the left-hand side of the rule is a POS, and there is no sibling to the left of constituent (A) in the derivation, then the algorithm takes the right branch of the decision tree to decide what value to return; otherwise the left branch.', 'Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown']",Method_Citation
5,J01-2004,2001a,7,P04-1015,0,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","[361, 345, 278]","['One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.', 'This is not surprising, since our conditioning information is in many ways orthogonal to that of the trigram, insofar as it includes the probability mass of the derivations; in contrast, their model in some instances is very close to the trigram, by conditioning on two words in the prefix string, which may happen to be the two adjacent words.', 'Section 22 (41,817 words, 1,700 sentences) served as the development corpus, on which the parser was tested until stable versions were ready to run on the test data, to avoid developing the parser to fit the specific test data.']",Method_Citation
6,J01-2004,2001a,9,P04-1015,0,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","[29, 178, 156]","['In such a scenario, there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule.', 'If the left-hand side of the rule is a POS, and there is no sibling to the left of constituent (A) in the derivation, then the algorithm takes the right branch of the decision tree to decide what value to return; otherwise the left branch.', 'That is, the number of distinct nonterminals grows to include the composite labels; so does the number of distinct productions in the grammar.']",Method_Citation
7,J01-2004,"Roark, 2001",10,P05-1022,0,0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","[9, 3, 267]","['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.']",Method_Citation
8,J01-2004,"Roark, 2001",11,P05-1022,0,0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","[297, 167, 216]","['The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.', 'In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version, we introduce the function constituent (A) for every nonterminal in the factored grammar Gf, which is simply the label of the constituent whose factorization results in A.', 'The derivation D consists of a sequence of rules used from G. The stack S contains a sequence of nonterminal symbols, and an end-of-stack marker $ at the bottom.']",Method_Citation
9,J01-2004,"Roark, 2001",12,P05-1022,0,0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","[195, 112, 188]","['In other words, this attempts to capture a certain amount of parallelism between the expansions of conjoined categories.', 'Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state, and conditioning on those entries in a way similar to an n-gram.', 'To understand why this is the case, one need merely to think of VP expansions.']",Method_Citation
10,J01-2004,"Roark, 2001",13,P04-1006,0,0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","[160, 370, 65]","['Each of these functions is an algorithm for walking the provided tree and returning a value.', 'We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.', 'We will adopt the convention that an explicit beginning of string symbol, (s), and an explicit end symbol, (/s), are part of the vocabulary, and a string wg is a complete string if and only if Tao is (s) and tv, is (/s).']",Method_Citation
11,J01-2004,"Roark, 2001a",14,P05-1063,0,0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","[8, 2, 19]","['The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.', 'A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.']",Method_Citation
12,J01-2004,"Roark, 2001",15,W10-2009,0,0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","[100, 299, 313]","['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'Furthermore, this is quite a large beam (see discussion below), so that very large improvements in efficiency can be had at the expense of the number of analyses that are retained.', 'By utilizing a figure of merit to identify promising analyses, we are simply focusing our attention on those parses that are likely to have a high probability, and thus we are increasing the amount of probability mass that we do capture, of the total possible.']",Method_Citation
13,J01-2004,2001,17,D09-1034,0,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","[341, 283, 8]","['Trigrams and bigrams were binned by the total count of the conditioning words in the training corpus, and maximum likelihood mixing coefficients were calculated for each bin, to mix the trigram with bigram and unigram estimates.', 'Unlike the Roark and Johnson parser, however, our coverage did not substantially drop as the amount of conditioning information increased, and in some cases, coverage improved slightly.', 'The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.']",Method_Citation
14,J01-2004,2001,18,D09-1034,0,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","[115, 245, 341]","['The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.', 'The empirical results will be presented in three stages: (i) trials to examine the accuracy and efficiency of the parser; (ii) trials to examine its effect on test corpus perplexity and recognition performance; and (iii) trials to examine the effect of beam variation on these performance measures.', 'Trigrams and bigrams were binned by the total count of the conditioning words in the training corpus, and maximum likelihood mixing coefficients were calculated for each bin, to mix the trigram with bigram and unigram estimates.']",Method_Citation
15,J01-2004,2001,19,D09-1034,0,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","[84, 351, 212]","['We will use this notion in our conditional probability model, and it is also useful for understanding some of the previous work in this area.', 'We did this for the first 10 sentences in the test corpus, a total of 213 words (including the end-of-sentence markers).', 'We will then present the algorithm in terms of this relation.']",Method_Citation
16,J01-2004,2001,20,D09-1034,0,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","[310, 29, 217]","[""This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Since RV is produced by expanding only analyses on priority queue H;', the set of complete trees consistent with the partial derivations on priority queue Ht is a subset of the set of complete trees consistent with the partial derivations on priority queue HT'', that is, the total probability mass represented by the priority queues is monotonically decreasing."", 'In such a scenario, there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule.', 'The probability PD is the product of the probabilities of all rules in the derivation D. F is the product of PD and a look-ahead probability, LAP(S,w,), which is a measure of the likelihood of the stack S rewriting with w, at its left corner.']",Method_Citation
