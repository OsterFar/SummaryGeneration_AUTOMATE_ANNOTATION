,Reference Article,Citation Marker,Citance Number,Citing Article,Citation Marker Offset,Citation Offset,Citation Text,Reference Offset,Reference Textt,Discourse Facet
0,A00-2018,"Charniak, 2000",2,N10-1002,0,0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","[91, 113, 90]","['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']",Method_Citation
1,A00-2018,"Charniak, 2000",3,W11-0610,0,0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","[133, 5, 180]","['One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.', ""From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.""]",Method_Citation
2,A00-2018,"Charniak, 2000",4,W06-3119,0,0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","[9, 93, 110]","['The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.', 'This allows the second pass to see expansions not present in the training corpus.', 'In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.']",Method_Citation
3,A00-2018,"Charniak, 2000",5,N03-2024,0,0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","[103, 174, 63]","['In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with < 2 cross brackets (2CB).', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.', ""As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la).""]",Method_Citation
4,A00-2018,"Charniak, 2000",6,N06-1039,0,0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","[189, 56, 54]","['Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.', 'In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.', ""The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c).""]",Method_Citation
5,A00-2018,2000,7,C04-1180,0,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","[1, 54, 154]","['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.', ""The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c)."", 'But nps and vps can occur with np and vp parents in non-coordinate structures as well.']",Method_Citation
6,A00-2018,"Charniak, 2000",8,W05-0638,0,0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","[117, 135, 109]","['Also, the earlier parser uses two techniques not employed in the current parser.', ""In contrast, the current parser first guesses the head's pre-terminal, then the head, and then the expansion."", ""Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9].""]",Method_Citation
7,A00-2018,"Charniak, 2000",9,P05-1065,0,0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","[91, 88, 81]","['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.', 'Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.']",Method_Citation
8,A00-2018,"Charniak, 2000",10,P05-1065,0,0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","[131, 95, 120]","['This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.', 'We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.', 'Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.']",Method_Citation
9,A00-2018,2000,11,P04-1040,0,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","[150, 180, 133]","['The second modification is the explicit marking of noun and verb-phrase coordination.', ""From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head."", 'One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.']",Method_Citation
10,A00-2018,2000,12,P04-1040,0,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","[56, 35, 155]","['In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.', 'In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].', 'For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.']",Method_Citation
11,A00-2018,2000,13,P04-1040,0,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","[70, 35, 188]","['One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.', 'In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].', 'Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming.']",Method_Citation
12,A00-2018,2000,17,N06-1022,0,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","[150, 173, 91]","['The second modification is the explicit marking of noun and verb-phrase coordination.', 'However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser.', 'As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']",Method_Citation
13,A00-2018,2000,18,N06-1022,0,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","[154, 54, 158]","['But nps and vps can occur with np and vp parents in non-coordinate structures as well.', ""The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c)."", 'A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly.']",Method_Citation
14,A00-2018,"Charniak, 2000",19,H05-1035,0,0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","[86, 136, 58]","['Naturally, the distributions required by Equation 7 cannot be used without smoothing.', 'It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].', 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.']",Method_Citation
15,A00-2018,2000,20,P04-1042,0,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","[176, 31, 49]","['That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.', 'Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.', 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.']",Method_Citation
